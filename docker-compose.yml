version: '3.8'

services:
  # Ollama - Servidor local de modelos de IA
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    restart: unless-stopped
    networks:
      - ai-network

  # LobeChat - Interface web para chat com IA
  lobe-chat:
    image: lobehub/lobe-chat:latest
    container_name: lobe-chat
    ports:
      - "3210:3210"
    environment:
      - OLLAMA_PROXY_URL=http://ollama:11434
      - OPENAI_API_KEY=sk-1234567890abcdef
      - OPENAI_API_BASE_URL=http://ollama:11434
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODELS=gemma3:1b,llama3.2:latest,qwen2.5:3b,deepseek-r1:1.5b
    volumes:
      - lobe_chat_data:/app/data
    restart: unless-stopped
    networks:
      - ai-network
    depends_on:
      - ollama

  ollama-init:
    image: alpine:latest
    container_name: ollama-init
    volumes:
      - ollama_data:/root/.ollama
    command: >
      sh -c "
        apk add --no-cache curl &&
        sleep 10 &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"gemma3:1b\"}' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"llama3.2:latest\"}' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"qwen2.5:3b\"}' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"deepseek-r1:1.5b\"}'
      "
    networks:
      - ai-network      
    depends_on:
      - ollama

volumes:
  ollama_data:
    driver: local
  lobe_chat_data:
    driver: local

networks:
  ai-network:
    driver: bridge 