# üöÄ Guia de In√≠cio R√°pido

Este guia te ajudar√° a configurar e usar o AI Integration Lab em poucos minutos.

## üìã Pr√©-requisitos

- Docker e Docker Compose instalados
- Python 3.8+ (para exemplos Python)
- Node.js (para exemplos JavaScript)
- Git

## ‚ö° Configura√ß√£o R√°pida

### 1. Clone o reposit√≥rio
```bash
git clone https://github.com/seu-usuario/ai-integration-lab.git
cd ai-integration-lab
```

### 2. Configure as vari√°veis de ambiente
```bash
cp env.example .env
# Edite o arquivo .env se necess√°rio
```

### 3. Inicie o Ollama
```bash
docker-compose up -d ollama
```

### 4. Verifique se est√° funcionando
```bash
curl http://localhost:11434/api/tags
```

## üß™ Primeiros Testes

### Teste via curl
```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama2",
    "prompt": "Ol√°! Como voc√™ pode me ajudar?",
    "stream": false
  }'
```

### Teste via Python
```bash
pip install -r requirements.txt
python examples/python/ollama_client.py
```

### Teste via JavaScript
```bash
node examples/javascript/ollama_client.js
```

### Teste via Bash
```bash
chmod +x examples/bash/ollama_queries.sh
./examples/bash/ollama_queries.sh
```

## üìö Pr√≥ximos Passos

1. **Explore os exemplos**: Navegue pela pasta `examples/` para ver diferentes formas de integra√ß√£o
2. **Experimente diferentes modelos**: Use `docker exec -it ollama ollama list` para ver modelos dispon√≠veis
3. **Crie seus pr√≥prios exemplos**: Use os templates fornecidos como base
4. **Contribua**: Adicione novos exemplos ou melhore os existentes

## üîß Configura√ß√µes Avan√ßadas

### Usando Redis para cache
```bash
docker-compose --profile cache up -d
```

### Usando Nginx como proxy
```bash
docker-compose --profile proxy up -d
```

### Baixando modelos espec√≠ficos
```bash
docker exec -it ollama ollama pull llama2:7b
docker exec -it ollama ollama pull codellama:7b
```

## üÜò Solu√ß√£o de Problemas

### Ollama n√£o inicia
- Verifique se a porta 11434 est√° livre
- Execute `docker-compose logs ollama` para ver logs
- Verifique se Docker tem permiss√µes adequadas

### Erro de conex√£o
- Verifique se o container est√° rodando: `docker ps`
- Teste a conectividade: `curl http://localhost:11434/api/tags`
- Reinicie o container: `docker-compose restart ollama`

### Modelo n√£o encontrado
- Liste modelos dispon√≠veis: `docker exec -it ollama ollama list`
- Baixe um modelo: `docker exec -it ollama ollama pull llama2`

## üìñ Recursos Adicionais

- [Documenta√ß√£o oficial do Ollama](https://ollama.ai/docs)
- [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Exemplos de prompts](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) 